{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e257ed-3df7-4157-b785-b0d201dfe274",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cff3f0-67b9-470f-9206-3a0c7a534df8",
   "metadata": {},
   "source": [
    "Vamos a ejecutar Spark sobre un solo nodo. No tendremos distribución ni HDFS, pero podremos mostrar _cómo_ se escribe código en este _framework_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f0988-f94a-4b68-91cd-e5b0b6bb01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1fb9b-a534-41d8-94af-90e1716e5da0",
   "metadata": {},
   "source": [
    "# 1. Leyendo o escribiendo datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317dc0d-a5ff-4361-b937-683791077e04",
   "metadata": {},
   "source": [
    "El principal \"punto de entrada\" a Spark es el objeto global `sc`, el `sparkContext`, o contexto de Spark. El flujo típico de un programa de Spark es el siguiente:\n",
    "\n",
    "1. se leen datos -_big_ (desde almacenamiento distribuido) o _small_ (desde nuestro cliente)-  *hacia* Spark.\n",
    "2. se realizan transformaciones y operaciones.\n",
    "3. se producen resultados *desde* Spark hacia el almacenamiento distribuido o que se traen al cliente.\n",
    "\n",
    "Vamos a leer una lista de Python con 3 elementos hacia Spark usando `parallelize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577ff08-4f44-465e-b4b3-8cdedd0f3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [1, 2, 3]\n",
    "l = sc.parallelize(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3f6d8-3fc3-4eb1-afe9-267aa7aa4dbf",
   "metadata": {},
   "source": [
    "¿Qué contiene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef310ec-d7ac-4303-9729-1a1eaa631734",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5974c4-4dad-4d4a-857f-86f5e002925f",
   "metadata": {},
   "source": [
    "Un `ParallelCollectionRDD` es la representación como `RDD` (luego veremos qué es esto) de una colección básica de Python (una lista, en este caso). No es un objeto al que podamos acceder directamente, solo tenemos una referencia al mismo, pero la colección (representada como estructura en Spark) \"vive\" gestionada por Spark.\n",
    "\n",
    "Si queremos retornar un objeto \"de vuelta\" al cliente usamos `collect`. ¡Ambas operaciones pueden ser muy costosas, sobre todo con datos de gran tamaño!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b4575-bde6-4d94-90b7-e73f5a6f5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = l.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c598c1-b1a7-4719-8bf0-2713f8ef99a0",
   "metadata": {},
   "source": [
    "Efectivamente, `result` es una lista con valores `[1, 2, 3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db725e2-f737-4131-9173-b1ddcf761314",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a57fc2-1f9b-4f8a-ba83-21359910810f",
   "metadata": {},
   "source": [
    "Por si queda alguna duda, las listas son **objetos distintos**. Al obtener la lista se crea un nuevo objeto en el cliente, que contiene los valores requeridos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f95773-f544-43d2-af12-5f157df57180",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex(id(result)) == hex(id(input_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a7605-603a-4eed-b979-7ffeb0da9b81",
   "metadata": {},
   "source": [
    "# 2. Qué es un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd82a57-4bd4-4c34-8a11-6605b8ce477f",
   "metadata": {},
   "source": [
    "Un `RDD` (_Resilient Distributed Dataset_) es la estructura básica en la que se almacenan los datos en Spark, y es la principal estructura en las primeras versiones de Spark.\n",
    "\n",
    "Como hemos dicho, un `RDD` \"vive\" gestionado por Spark. Así que podemos asumir que existe en un nodo y que, posiblemente está distribuido. \n",
    "\n",
    "**Conceptualmente** podemos pensar en un `RDD` como una lista de pares `(clave, valor)` inmutable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9962a-c7d0-4d5a-bf45-b3f70c8fbc43",
   "metadata": {},
   "source": [
    "En la práctica un RDD puede ni siquiera existir como estructura en memoria; en el ejemplo anterior la sucesión de dos instrucciones crea un DAG con el esquema de procesado que solo se ejecuta cuando se \"pide\" algo al cliente (en nuestro caso, al hacer collect de `l`. Vamos a ver esto con un ejemplo en el que introduciremos una serie de instrucciones nuevas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1b93e-3111-4257-8cab-f8887192a3a0",
   "metadata": {},
   "source": [
    "## 2.1 Contando palabras de \"El Quijote\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21380367-8656-4ce0-b2b2-a2a936da857b",
   "metadata": {},
   "source": [
    "Este es un ejemplo de uso de las primitivas de lectura de archivos, y nuestro primer programa map-reduce. Vamos a usar `textFile` para \"leer\" un archivo de texto del cliente hacia Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602146-9e49-416c-af86-79bdafd1c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote = sc.textFile(\"el_quijote.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377bd0f-a9a7-4655-a414-b465171d965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dad084-bfff-4b7d-98df-2b9764820f3a",
   "metadata": {},
   "source": [
    "Tenemos un `MapPartitionsRDD`. Se trata de un RDD de cadenas de caracteres, donde cada una de ellas es una línea. Vamos a ver esto usando la función `first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958397c-6d24-4e28-8c05-c6fed2c04077",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7ec4-a25e-488e-92c6-ea4f07ad420c",
   "metadata": {},
   "source": [
    "La instrucción `first` es útil si queremos mostrar parte del RDD y ver si contiene lo que debería. También podríamos hacer `take(N)` para obtener los `N` primeros elementos. En cualquier caso, hay que cuidar que esto no ejecute (aunque sea parcialmente) un grafo muy complicado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cd433-3fae-4273-a50e-a8ef8ec332c4",
   "metadata": {},
   "source": [
    "Con esto, hemos creado (y ejecutado) un DAG muy simple. Ahora vamos a contar palabras (lo que implica, como sabéis, una parte `map` y otra `reduce`). En Spark no es necesario escribir clases independientes para cada operación; incluso, ¡se puede hacer todo de una vez!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52083674-33bd-457c-bb5b-496640bf5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = quijote.flatMap(lambda line: line.upper().split(\" \"))\\\n",
    "                .map(lambda x: (x, 1))\\\n",
    "                .reduceByKey(lambda a, b: (a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d240be-fa16-4047-bca2-ac96b3366365",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd29fb7-1294-4cb0-91eb-39ff4d5184a1",
   "metadata": {},
   "source": [
    "Tenemos como resultado un `RDD`, pero, ¿está hecho el cálculo? Definitivamente **no**. Spark está esperando a que lancemos una instrucción que ejecute el grafo. Por lo general Spark devuelve referencias `lazy`, esto es, que solo tienen un cierto valor si se ejecutan todos los cálculos dependientes y se realiza una operación que verdaderamente requiere dicho valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ecafb-5044-451c-ad33-1222b142187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras.sortBy(lambda x: -x[1]).keys().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a09fa-4130-4444-8f81-623079f6bfd4",
   "metadata": {},
   "source": [
    "Lo de arriba calcula las 10 palabras más comunes en \"El Quijote\". Nótese el uso de `sortBy` con una función anónima: esto es una operación cara que por lo general implica `shuffles` y debe, por tanto evitarse al máximo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9041645-f424-4c80-a24e-07810f929942",
   "metadata": {},
   "source": [
    "No obstante, `sortBy` es \"tan eficiente como puede ser\":\n",
    "    - Primero se ordenan cada una de las particiones.\n",
    "    - Luego se producen los shuffles correspondientes, siguiendo los órdenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f57eb3-90b2-4557-882f-5bd88fcbcced",
   "metadata": {},
   "source": [
    "# 2.2. Una versión alternativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54da85c-778f-4cc2-9afc-4463a4e7ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([x.upper().split() for x in open(\"el_quijote.txt\")])\\\n",
    "    .flatMap(lambda x: [(y, 1) for y in x])\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .sortBy(lambda x: -x[1]).keys().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952977b1-7dd9-4063-8b12-f1d7b61ba845",
   "metadata": {},
   "source": [
    "**Para pensar:** ¿Cuál es la principal diferencia? ¿Por qué tarda más?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b625e-aa40-41a5-98bb-737c43d22189",
   "metadata": {},
   "source": [
    "# 3. Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61df63-b565-4cae-85fd-89d2a8ce7451",
   "metadata": {},
   "source": [
    "El uso de `RDDs` puede ser tedioso. Algunas de sus limitaciones:\n",
    "- No hay flexibilidad (son estructuras `(clave, valor)`).\n",
    "- Una vez creado un RDD no hay forma de conocer sus tipos. Esto puede inducir a error en operaciones complicadas, por ejemplo, en un `join`.\n",
    "- Cuando queremos escribir a cliente tenemos una estructura que hay que transformar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ac2df-2891-41bc-b228-354c20a783e9",
   "metadata": {},
   "source": [
    "Spark introdujo experimentalmente en su versión 1.6 los `DataFrames`. Estos son una abstracción sobre una tabla, con filas y columnas, similar a los Pandas `DataFrames`. Su interfaz es más sencilla, sobre todo si estamos familiarizados con las bases de datos. Vamos a revisitar nuestro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad3d93-0f2d-4acb-b909-1047224c413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d054e2d-2443-4b96-b655-cb9019ce5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote_l = sc.textFile(\"el_quijote.txt\")\\\n",
    "    .flatMap(lambda line: [(x, 1) for x in line.upper().split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d85884-9995-4253-9116-d4123588ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote_l.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e92d3-1000-4a0c-8059-85453fb000a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_sin_schema = spark.createDataFrame(quijote_l)\n",
    "df_quijote_sin_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815aa2ac-5f61-4b37-abcc-7a3ba914d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_sin_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351cc4d-8af5-43b9-8895-c7fd885bd7bd",
   "metadata": {},
   "source": [
    "¡Hemos creado nuestro primer `DataFrame`! Se muesltran dos columnas con tipos `string` y `bigint` (inferidos) y nombres poco inteligibles. Repitamos la operación aplicando un schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7ec74-aebb-4a04-9975-fbee8e2ebc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b78ed0-f63b-478b-8a37-5be0fcf9f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote = spark.createDataFrame(\n",
    "    quijote_l,\n",
    "    schema=StructType([StructField(\"palabra\", StringType()),\n",
    "                      StructField(\"frecuencia\", IntegerType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813073af-cdb2-4c75-a67d-b541e039d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14a7df-7b77-4931-b717-3f9cb0026e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d400bb7-e47b-44c6-9962-37599b4bedc8",
   "metadata": {},
   "source": [
    "¿Qué podemos hacer con un `DataFrame`? Vamos a mostrar las 10 palabras más frecuentes usando instrucciones \"tipo SQL\". Veamos la instruccion en 3 etapas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50678ae-98a5-4da2-a5e0-453def70812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177f776-e9ec-4b1a-ac9c-1ad377767a60",
   "metadata": {},
   "source": [
    "1. GROUPBY palabra + sum (¡igual que en SQL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb2e86-e6d8-435c-bf00-d5d19b088cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote.groupby(\"palabra\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e737a4-f4bf-4434-bf31-dfb97dcd9bf5",
   "metadata": {},
   "source": [
    "2. orderBy `sum(frecuencia)` (columna creada al sumar frecuencias) en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b50c76-f10c-4e00-bb9e-833c5a86814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote.groupby(\"palabra\").sum().orderBy(\n",
    "    col(\"sum(frecuencia)\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520736d-ac38-47e3-b0a9-3d2df4afb686",
   "metadata": {},
   "source": [
    "3. tomamos las 10 filas más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef428d87-ff9b-496d-ae2f-a1c109e061e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote.groupby(\"palabra\").sum().orderBy(\n",
    "    col(\"sum(frecuencia)\").desc()).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297a0ab-ba7a-4b71-8244-9f53d8288bcd",
   "metadata": {},
   "source": [
    "4. devolvemos el resultado al cliente a un `pandas DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81126b02-cf7c-4f98-91ca-01bdbb1432d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_pd = df_quijote.groupby(\"palabra\").sum()\\\n",
    "                          .orderBy(col(\"sum(frecuencia)\").desc())\\\n",
    "                          .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1f249-9e8c-46da-8d6d-da5d130f5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134c665-bc7a-4ace-b8b5-b0e5e4b33d5b",
   "metadata": {},
   "source": [
    "La interacción con `pandas` es muy conveniente. Desde Pandas podemos trabajar en el cliente (graficar, escribir a csv o a otros formatos, hacer operaciones no costosas, presentar los resultads, etc.).\n",
    "\n",
    "Obviamente, también podemos \"enviar\" un DataFrame de Pandas (usualmente un dataset mediano para interactuar con otro grande) hacia Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e35e73-823b-4dfe-9ac5-dad359eec4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_otro = spark.createDataFrame(df_quijote_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa90899-215b-4428-a0db-5d11e4350254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffcab4-e5bb-4e6c-bf48-c48bcda25626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_otro.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2a12d-2761-4258-94e2-6fb0744766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quijote_otro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc8dbe-6702-4b36-974e-e97cf75a1f71",
   "metadata": {},
   "source": [
    "# 4. Para saber más"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541550c-fe0f-46c5-9b43-894b8e87bc47",
   "metadata": {},
   "source": [
    "- La documentación de PySpark es muy completa, con tuturiales y una API detallada. https://spark.apache.org/docs/latest/api/python/.\n",
    "- El libro de Holden Karau \"High Performance Spark\" es una de las referencias más completas si quieres aprender qué ocurre dentro de Spark, saber cómo se optimizan consultas e ir a temas más avanzados: https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9fefa-2b00-4f7f-8944-d3f89b4c5d9e",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
